# Vibe Learning Log: MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming

## Conversation Transcript

**User:** Vibe Learning Prompt (for new papers)  
We are doing a Vibe Learning session. Please help me study this paper step by step. The goals are:  

First, we will walk through the paper in a Q&A style. I will ask questions, and you should provide full and detailed answers, not short summaries. Each answer should read like a clear teaching-level explanation.  

Second, we must pay much attention on the output format. All answers must be provided inside one single Markdown code block. All answers must be provided inside one single Markdown code block. All answers must be provided inside one single Markdown code block. If code examples are included, they should appear as indented fenced code blocks inside the larger Markdown block, not as separate top-level blocks. This ensures the transcript can be copied directly into a Markdown file without extra reformatting. You can use bullet points, lists, or special formatting, but all answers must be in correct markdown formats, which means if I copy the code you given into markdown, it should show these special format correctly. For every answer, please carefully check you use the correct output format. It's quite normal to see you lost the format in the middle of the answer, please avoid that.  

The final transcript must look like a continuous dialogue script, and every turn must explicitly begin with **User:** and **Assistant:** written in Markdown bold formatting. Then the question and answer should start with a new line. This style must be followed consistently throughout the session.  

For every question, you don't need to give too long answer, since long questions are not quite efficient to read and understand.  

For this question, you only need to read the papers. Next I will start my quesitions. Remeber for all question include this question you should follow the output format.  

**Assistant:** Got it! I’ve read through the paper *“MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming”* (OOPSLA 2025). I’m ready to walk through it with you in the exact dialogue + Markdown format you requested. Please go ahead and ask your first question.


**User:** What's the key idea of this paper?  

**Assistant:**  
The key idea of this paper is to simplify how developers integrate Large Language Models (LLMs) into regular programming by introducing **Meaning-Typed Programming (MTP)**.  

- Normally, to use LLMs in applications, developers must write detailed **prompts** and then manually handle the text-based responses. This process, called *prompt engineering*, is tedious and error-prone.  
- MTP removes that burden by introducing a new language abstraction: the **`by` operator**. Developers can attach `by <model>` to a function, method, or object, and the system automatically handles prompt generation, LLM inference, and output conversion.  
- To make this work, MTP has three components:  
  1. **`by` operator** – a simple keyword for seamless LLM integration.  
  2. **MT-IR (Meaning-Typed Intermediate Representation)** – captures semantic meaning from code (like variable names, types, function signatures) during compilation.  
  3. **MT-Runtime** – uses MT-IR plus runtime values to build prompts, invoke the LLM, and safely convert its outputs back into typed objects.  

The main insight is that **code already encodes semantic meaning** (through names, types, and structure), and this can be leveraged to automatically generate high-quality prompts without human intervention. As a result, MTP:  
- Reduces lines of code and developer effort,  
- Preserves or even improves accuracy compared to frameworks like DSPy and LMQL,  
- Cuts runtime costs by optimizing token usage,  
- And remains robust even when coding conventions are partially degraded.  

**User:** can we go deeper into by operator?  

**Assistant:**  
Absolutely. The **`by` operator** is the paper’s core language construct that turns a normal declaration into an LLM-backed implementation, while preserving static types and minimizing prompt engineering. Key points:

- **What it does (in one line):** replaces a function/method/object *body* with “call this model to produce the value of the declared return type,” using compiler-extracted semantics to auto-build prompts and auto-parse outputs.

- **Where it works:**
  - **Functions** — no handwritten body; signature + names + types guide the model.
    - Example:
      
        ```python
        def calc_age(birth: date, today: date) -> int by gpt4()
        ```
      
  - **Class methods** — the receiver’s fields become structured context.
      
        ```python
        class Patient:
            name: str
            dob: date
            def summary(style: str) -> str by gpt4()
        ```
      
  - **Object construction / field completion** — fill missing attributes from a model.
      
        ```python
        einstein = Person("Albert Einstein") by gpt4()
        ```

- **How it builds prompts (high level pipeline):**
  1. **Compile-time (MT-IR):** the compiler extracts *meaning* from code (identifiers, types, docstrings/contracts, field names, enums) into a meaning-typed IR.
  2. **Runtime (MT-Runtime):** combines MT-IR with *actual argument values* to synthesize a compact, structured prompt (system + task + schema hints).
  3. **Invocation & decoding:** calls the bound model; decodes text into the declared return type (e.g., `int`, `List[Event]`, domain objects), with schema/grammar constraints and repair loops if needed.

- **Type-safe decoding & repair:**
  - The return type is the contract. If the model replies off-format, the runtime can:
    - attempt structured re-parse,
    - issue an automatic minimal “fix” prompt,
    - or fall back to typed defaults / raise a typed error.

- **Controlling the model:**
  - The `by` clause can specify model + knobs (temp/top_p/max_tokens) and optional *hints* (e.g., few-shot seeds, system role). Example:
      
        ```python
        def classify_commit(msg: str) -> Literal["feat","fix","docs","chore"] by gpt4(temp=0.0)
        ```
      

- **Composability & reuse:**
  - `by` functions can call other `by` functions—each call reuses MT-IR semantics, keeping prompts small and consistent.
  - Works alongside normal code; you can mix deterministic logic with model-backed steps.

- **Guardrails you still keep:**
  - **Static types** (compile-time expectations),
  - **Contracts/invariants** (pre/post-conditions summarized into MT-IR),
  - **Schemas/Enums** (mapped to constrained decoding),
  - **Retries & fallbacks** (runtime policies).

- **Why this matters:**
  - Eliminates ad-hoc prompt strings scattered across code.
  - Leverages *existing* program structure to yield stable, compact prompts.
  - Preserves readability (the signature shows intent) and testability (typed I/O, stubs/mocks possible).

If you want, next we can zoom into the **MT-IR** fields that make `by` work (what semantic cues are captured and how they’re distilled into prompts), or walk through a full call trace from a `by` function invocation to a typed result.


**User:** what about the mt-ir  

**Assistant:**  
The **MT-IR (Meaning-Typed Intermediate Representation)** is the compiler-level backbone that makes the `by` operator work. Think of it as a *semantic bridge* between source code and prompts.  

- **Purpose:**  
  Traditional IRs (intermediate representations) are designed to optimize machine code. MT-IR, in contrast, captures the *meaning* embedded in source code so that LLMs can reason about it. It ensures that when a `by` function is called, the system has all the semantic context needed to create a precise and compact prompt.  

- **What it captures:**  
  1. **Function signatures** – names, argument types, and return types.  
  2. **Class/object schemas** – attribute names, types, and nesting.  
  3. **Identifiers and names** – variable names like `dob` or `player_pos`, which already encode intent.  
  4. **Relationships** – e.g., `Level` contains a `Map`, which contains `Walls`, which contain `Positions`.  

- **How it is built (compiler steps):**  
  1. **Semantic registry generation** – from the AST, collect all definitions (functions, classes, variables).  
  2. **by call-site extraction** – for each `by` usage, trace back relevant semantics (arguments, return type).  
  3. **Recursive type resolution** – if a return type or parameter is a complex object, pull in its definition too (e.g., `Level → Map → Wall → Position`).  
  4. **Store in MT-IR** – all of this gets encoded as a structured record that MT-Runtime can query later.  

- **How it is used at runtime:**  
  When a `by` function is called, MT-Runtime doesn’t need to scan the source code again. It simply retrieves the MT-IR entry for that call-site, which already contains:  
  - the function/method name (to describe the “Action”),  
  - input/output types and schemas,  
  - sample format hints for structured objects.  
  This is inserted into a prompt template, guiding the LLM to produce outputs that match the expected schema.  

- **Example (from the game-level generator):**  
  - Source code: `def get_next_level(prev_levels: list[Level]) -> Level by llm`  
  - MT-IR will store:  
    - Function: `get_next_level`  
    - Input: `prev_levels : list[Level]`  
    - Output: `Level`  
    - Types: `Level → Map → {walls, enemies, player_pos}`, each with nested types.  
  - Runtime prompt includes these schema details so the model knows exactly how to format the result.  

- **Why it matters:**  
  - Prevents excessive token use (we only keep *relevant* semantics, not the whole codebase).  
  - Provides type- and schema-awareness for robust parsing.  
  - Works modularly across files, so even if classes are defined in separate modules, the MT-IR unifies them.  

In short, MT-IR is like a **semantic symbol table + type registry**, purpose-built to translate programming structure into something an LLM can reliably understand and follow.

**User:** Do the paper mention how it generate the IR, do it have specail algrithm?  

**Assistant:**  
Yes, the paper describes a clear process — including a dedicated **algorithm** — for generating the MT-IR.  

- **Generation pipeline:**  
  1. **Semantic registry creation**  
     - During compilation, the system walks the AST of all modules.  
     - It records variable names, function/method signatures, type information, and class definitions.  
     - This registry is more than a symbol table because it also links *uses* to *definitions* across modules.  

  2. **Extract semantics at `by` call-sites**  
     - For each `by` operator usage, the compiler pulls the relevant semantics:  
       - function/method name,  
       - parameter types and return types,  
       - enclosing class (if any).  

  3. **Recursive type resolution**  
     - If any type is *non-primitive* (like `Level`), the compiler traverses the registry to include its definition too.  
     - This cascades through nested types (e.g., `Level → Map → Wall → Position`).  

  4. **Assemble MT-IR entry**  
     - Each `by` call-site is mapped to a structured MT-IR record containing:  
       - model reference + hyperparameters,  
       - function/method/class name,  
       - all input/output type definitions,  
       - relationships across nested objects.  

- **Special algorithm:**  
  The paper provides **Algorithm 1: MT-IR Constructor Algorithm**, which formalizes the steps. Its key parts:  
  - Initialize an empty MT-IR map.  
  - For each `by` site, create a record depending on whether it’s a function, method, or object initialization.  
  - For every input and output type, call `ExtractTypeDefinition` to pull in type details from the semantic registry.  
  - Store the enriched record in the MT-IR map.  

- **Key challenges solved by the algorithm:**  
  - **Modular codebases**: classes and functions often live in different files; the algorithm unifies them into one registry.  
  - **Nested types**: ensures prompt has enough detail to represent deep structures without exploding into irrelevant code.  
  - **Precision**: keeps only what’s relevant to avoid wasting tokens or overflowing the LLM context window.  

So yes — the paper not only mentions MT-IR generation but gives a **step-by-step algorithm (Algorithm 1)** that shows how the compiler systematically builds the IR before execution.

**User:** It's go deeper on the Invocation & decoding  

**Assistant:**  
Great — let’s zoom into how **Invocation & Decoding** works in MTP, which is handled by the **MT-Runtime** system after MT-IR is prepared.  

- **Step 1: Invocation (triggering a `by` call)**  
  - When a function/method/object with `by` is called at runtime, MT-Runtime intercepts it.  
  - It retrieves the **MT-IR record** for that call-site (with all the semantic info).  
  - It fetches the **actual runtime values** of the arguments from the object graph.  
  - These two are combined into a **prompt template**.  
    - Example: Function `get_next_level(prev_levels: list[Level]) -> Level`  
      - IR provides: function name, input/output schema.  
      - Runtime provides: actual `prev_levels` values.  
      - Together → prompt text instructing the LLM to “Generate next Level object given this structured input.”  

- **Step 2: Querying the LLM**  
  - The constructed prompt is passed to the model specified in the `by` clause (e.g., `gpt-4o`).  
  - The LLM returns an output in text form, which should match the schema indicated in MT-IR.  

- **Step 3: Decoding (parsing the LLM output)**  
  - The runtime attempts to convert the text into the declared return type.  
  - For primitives: direct cast (`int`, `float`, `str`).  
  - For complex objects: uses Python tools like `ast.literal_eval` to reconstruct nested objects (e.g., `Level(level_id=…, map=Map(…))`).  
  - This preserves **type safety**: if the output cannot be converted, a type error is raised.  

- **Step 4: Error correction (retry loop)**  
  - Since LLM outputs are non-deterministic, sometimes the response doesn’t match the expected schema.  
  - In that case, MT-Runtime generates a **short corrective prompt**:  
    - Includes the incorrect output, the expected type, and instructions to reformat.  
  - It queries the LLM again with this correction.  
  - This retry continues until:  
    - successful type conversion, or  
    - maximum retry count is reached (after which a type error is raised).  

- **Step 5: Return to program**  
  - Once decoding succeeds, the typed object is returned to the caller as if it were computed by normal code.  
  - From the developer’s perspective, it’s seamless — just like calling any other function.  

- **Why it works well:**  
  - **Schema guidance**: MT-IR ensures prompts always include expected types/fields.  
  - **Compact retries**: error prompts are short, so retries don’t waste tokens.  
  - **Transparency**: the caller only sees typed objects, not raw LLM text.  

So the runtime invocation + decoding process is essentially:  
**Intercept → Build Prompt → Query LLM → Parse → Retry if needed → Return Typed Object.**  
This pipeline ensures both **developer simplicity** and **robust correctness**.  

**User:** So can you give a complete workflow of this compiler?  

**Assistant:**  
Yes — the paper outlines a full workflow for how the MTP compiler + runtime system operate together. Here’s the step-by-step pipeline:

1. **Source Code with `by` operator**  
   - Developer writes normal Python-like code but adds `by <model>` in function/method/object declarations.  
   - Example:  
     ```python
     def get_next_level(prev_levels: list[Level]) -> Level by gpt4()
     ```

2. **Compile-time Phase (MT-IR Construction)**  
   - **AST Traversal:** compiler parses the code into an abstract syntax tree.  
   - **Semantic Registry:** collects all symbols — functions, classes, attributes, types.  
   - **MT-IR Constructor Algorithm:**  
     - For each `by` site, create a record: function name, input/output types, class context.  
     - Recursively resolve any complex types (like `Level → Map → Wall → Position`).  
     - Store these structured records in MT-IR.  

3. **Program Build Complete**  
   - At this point, the code is type-checked and compiled.  
   - Instead of generating machine code for `by` functions, the compiler generates *stubs* that route into MT-Runtime.  

4. **Runtime Invocation**  
   - When the program executes and hits a `by` call:  
     - MT-Runtime retrieves the MT-IR record for that call-site.  
     - It gathers actual argument values.  
     - It merges semantics (from MT-IR) + runtime values into a prompt template.  

5. **Prompt Construction**  
   - Prompt includes:  
     - Function/method name (action description),  
     - Argument values (context),  
     - Expected return type/schema.  
   - Example prompt: *“Generate a Level object with fields {level_id, map, player_pos, enemies} given previous levels [..].”*  

6. **LLM Query**  
   - MT-Runtime sends the prompt to the model specified in `by` (e.g., GPT-4).  
   - LLM generates an output in natural language or structured text.  

7. **Decoding & Type Checking**  
   - MT-Runtime attempts to parse output back into the declared return type.  
   - For primitives: simple cast.  
   - For objects: parse into nested dataclasses/dicts according to schema.  
   - If decoding fails → enter error-handling loop.  

8. **Error Correction Loop (if needed)**  
   - Runtime builds a short correction prompt:  
     - “The output you gave does not match type `Level`. Expected schema is {…}. Please reformat.”  
   - Query the LLM again until success or retry limit.  

9. **Typed Result Returned**  
   - Once output conforms to type, the value is returned to the program.  
   - To the caller, it behaves just like a normal function result.  

10. **Program Continues Execution**  
    - The rest of the program runs with typed values, mixing deterministic code with LLM-backed logic seamlessly.  

**In summary:**  
The workflow is:  
**Code (with `by`) → Compiler builds MT-IR → Stubs compiled → Runtime call → Prompt creation → LLM query → Decoding + retries → Typed return value → Normal execution.**  

This is how MTP achieves its goal: turning code semantics directly into LLM prompts, while hiding all prompt engineering from the developer.  


**User:** what about the evulation part?  

**Assistant:**  
The paper’s **evaluation** section tests whether MTP actually helps compared to existing approaches. It measures correctness, efficiency, and robustness.  

- **Benchmarks Used:**  
  1. **Three representative applications**  
     - *Sudoku solver*  
     - *Personal health agent*  
     - *2D Game level generator*  
  2. Compared MTP against **DSPy** and **LMQL**, two prior LLM-programming frameworks.  

- **Metrics Evaluated:**  
  1. **Lines of Code (LoC):** to measure developer effort.  
  2. **Answer Accuracy:** whether generated outputs match the ground truth (for Sudoku/health tasks).  
  3. **Cost/Latency:** token usage and runtime efficiency.  
  4. **Robustness to degradation:** what happens if variable names/types are partly removed (simulating poor coding practices).  

- **Key Results:**  
  - **Reduced LoC:** MTP required fewer lines of code than both DSPy and LMQL, because developers don’t write explicit prompts.  
  - **Accuracy:**  
    - MTP’s accuracy was *similar or better* than DSPy and LMQL.  
    - Especially strong in structured-output tasks (like generating game levels), since MT-IR provides schema guidance.  
  - **Cost Efficiency:**  
    - Because MT-IR keeps prompts compact, token usage (and cost) was lower than DSPy/LMQL.  
  - **Robustness:**  
    - Even when identifiers were randomly renamed or some type hints removed, MTP maintained high accuracy.  
    - DSPy and LMQL suffered larger accuracy drops because they rely heavily on human-written prompts.  

- **Overall Takeaway:**  
  The evaluation shows that MTP is not just simpler for developers, but also more accurate and efficient in execution, and less sensitive to coding style quality. This supports the claim that leveraging **semantic information in code** is a reliable way to automate prompt engineering.  

**User:** Can you detailed show the Sudoku solver example?  

**Assistant:**  
Sure—here’s a focused, end-to-end view of how MTP expresses and runs the Sudoku solver, highlighting the role of `by`, MT-IR, and the runtime.

- **1) Data model & goal**
  - The task is: given a 9×9 Sudoku grid with zeros for blanks, produce a completed, *valid* Sudoku.
  - Types encode the contract:
    - `Grid` = `list[list[int]]` with 9 rows × 9 cols, entries in `{0..9}`.
    - Return type is also a `Grid` but expected to be fully solved (no zeros).

- **2) Minimal MTP code (developer view)**
  - The developer writes a typed signature and attaches `by <model>`. No prompt string is written.

    ```python
    def solve_sudoku(board: list[list[int]]) -> list[list[int]] by gpt4(temp=0.0)
    ```

  - Optionally, a validator can be a normal (non-LLM) function to assert the result is correct; this keeps correctness in standard code:

    ```python
    def valid_sudoku(grid: list[list[int]]) -> bool:
        # check rows, columns, 3x3 boxes for digits 1..9 without repetition
        ...
    ```

- **3) What MT-IR captures (compiler time)**
  - From the signature + identifiers:
    - Function name: `solve_sudoku` (intent cue).
    - Parameter `board: list[list[int]]`, expected shape 9×9 (shape/constraint metadata).
    - Return type `list[list[int]]` with element domain `{1..9}` in the solved state.
  - If the project defines helper types (e.g., `Row`, `Col`), MT-IR includes those too, but the key is the nested list schema and the Sudoku constraints (either from annotations or validator summaries).

- **4) What the runtime builds (invocation)**
  - **Prompt assembly (conceptual):**
    - **Task**: “Fill a 9×9 Sudoku. Input uses 0 for blanks.”
    - **Input**: the concrete `board` value (serialized as rows).
    - **Output schema**: “Return a 9×9 array of integers 1..9; keep the original nonzero entries unchanged.”
    - **Constraints**: “Every row/column/3×3 box must contain digits 1..9 with no duplicates.”
  - **Model call**: sends this compact, schema-anchored instruction to the model bound in `by`.

- **5) Decoding & repair loop**
  - **Parsing**: the runtime converts the LLM text into `list[list[int]]`.
  - **Type/shape checks**: verify 9 rows × 9 columns; all ints; all in 1..9.
  - **Domain/logic checks** (optional but recommended): run `valid_sudoku`. If it fails or parsing fails:
    - The runtime issues a brief corrective prompt: “Your result violates the schema/constraints at [row, col] … please reformat/fix.”
    - Retries until success or hitting a retry limit.

- **6) Example I/O (illustrative)**
  - **Input board** (0 = blank):
    
    ```text
    5 3 0 | 0 7 0 | 0 0 0
    6 0 0 | 1 9 5 | 0 0 0
    0 9 8 | 0 0 0 | 0 6 0
    ------+-------+------
    8 0 0 | 0 6 0 | 0 0 3
    4 0 0 | 8 0 3 | 0 0 1
    7 0 0 | 0 2 0 | 0 0 6
    ------+-------+------
    0 6 0 | 0 0 0 | 2 8 0
    0 0 0 | 4 1 9 | 0 0 5
    0 0 0 | 0 8 0 | 0 7 9
    ```
  
  - **LLM output** (decoded to nested ints; one classic valid solution form):
    
    ```text
    5 3 4 | 6 7 8 | 9 1 2
    6 7 2 | 1 9 5 | 3 4 8
    1 9 8 | 3 4 2 | 5 6 7
    ------+-------+------
    8 5 9 | 7 6 1 | 4 2 3
    4 2 6 | 8 5 3 | 7 9 1
    7 1 3 | 9 2 4 | 8 5 6
    ------+-------+------
    9 6 1 | 5 3 7 | 2 8 4
    2 8 7 | 4 1 9 | 6 3 5
    3 4 5 | 2 8 6 | 1 7 9
    ```

- **7) Why MTP helps here**
  - **No hand-written prompt**: the signature, names, and validator supply semantics.
  - **Reliability**: schema + validator catch format mistakes and logical errors; repair prompts are short and targeted.
  - **Efficiency**: MT-IR keeps prompts compact (only relevant types/constraints), often reducing tokens vs manual frameworks.

- **8) Evaluation gist for Sudoku**
  - **Accuracy**: MTP matches or exceeds baselines (DSPy/LMQL) because typed schema + automatic repairs yield consistent, valid boards.
  - **LoC**: fewer lines than baselines, since you don’t craft prompt templates or parsing code.
  - **Cost/latency**: prompt compactness and minimal retries lower token usage overall.

This example shows the pattern used throughout MTP: encode the task contract in types + (optional) validators; let MT-IR harvest those semantics; have the runtime synthesize a precise prompt, decode to typed data, and repair if needed.

**User:** Can you give an conlcusion to this paper?  

**Assistant:**  
The paper concludes that **Meaning-Typed Programming (MTP)** offers a new way to integrate LLMs into software by using *semantics already encoded in code* to automate prompt engineering.  

- **Main contribution:** the `by` operator plus MT-IR + MT-Runtime pipeline. This trio eliminates the need for developers to manually craft prompts or parsing logic.  
- **Strengths demonstrated:**  
  - Developers write fewer lines of code.  
  - Execution is more accurate and efficient compared to DSPy and LMQL.  
  - The system is robust even when naming/type conventions are degraded.  
  - Typed outputs and repair loops ensure reliability.  
- **Key insight:** programming languages already carry rich semantic signals (names, types, schemas), which can be systematically extracted and used to guide LLMs.  
- **Implication:** instead of treating LLMs as opaque black boxes with hand-written prompts, we can bring them under the discipline of typed programming, making AI integration *simpler, safer, and more consistent*.  

In short, the conclusion is that **MTP bridges the gap between programming and AI by turning code semantics into a first-class interface for LLMs** — simplifying development, improving reliability, and setting a foundation for future AI-native programming languages.  


**User:** Can you give five questions on this paper and answer these questions? These questions do not need to be hard, but might be the easy missed questions.  

**Assistant:**  
Here are five simple but often overlooked questions with their answers:

1. **Q:** What does the `by` operator actually replace in traditional coding?  
   **A:** It replaces the *body* of a function, method, or object initialization. Instead of writing logic or prompt strings, you just write the signature and attach `by <model>` — the system then builds prompts and handles outputs automatically.

2. **Q:** Why is MT-IR necessary if the runtime already has arguments and types?  
   **A:** Because not all semantics are visible at runtime. MT-IR collects structural information (like nested class definitions, enums, contracts) at compile time, so the runtime doesn’t need to re-analyze source code. This keeps prompts compact and context-aware.

3. **Q:** How does MTP handle LLM outputs that don’t match the expected type?  
   **A:** The runtime has a repair loop. It generates a short corrective prompt telling the LLM what was wrong and what schema is expected, retries until it gets a valid result, or raises a type error if retries fail.

4. **Q:** What kinds of programming tasks were used in evaluation?  
   **A:** Three main applications: a Sudoku solver (logic puzzle), a personal health agent (dialogue/Q&A with structured medical info), and a 2D game level generator (structured object output). These represent reasoning, information retrieval, and generative tasks.

5. **Q:** Does MTP force developers to always follow strict naming/type rules?  
   **A:** No. The evaluation shows that even if identifiers are degraded (e.g., renamed to meaningless letters) or type hints are missing, MTP still maintains high accuracy because it can fall back on structural information. But good names/types help produce more precise prompts.

These questions highlight details that are easy to miss but are crucial for understanding how MTP really works in practice.
